services:
  keys:
    image: node:20-bookworm-slim
    working_dir: /scripts
    volumes:
      - ./scripts:/scripts:ro
      - keys:/keys
    command: ["node", "/scripts/gen-keys.js", "/keys/keys.env"]

  model:
    image: curlimages/curl:8.5.0
    user: "0:0"
    volumes:
      - ./models:/models
    environment:
      MODEL_URL: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf
    entrypoint:
      [
        "sh",
        "-c",
        "if [ ! -f /models/tinyllama.gguf ]; then echo 'downloading model'; curl -L \"$$MODEL_URL\" -o /models/tinyllama.gguf; else echo 'model already present'; fi",
      ]

  router:
    build:
      context: ../..
      dockerfile: services/router/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      ln-adapter:
        condition: service_started
    environment:
      ROUTER_ENDPOINT: http://router:8080
      ROUTER_PORT: 8080
      ROUTER_REQUIRE_PAYMENT: "true"
      ROUTER_LN_INVOICE_URL: http://ln-adapter:4000/invoice
      ROUTER_LN_VERIFY_URL: http://ln-adapter:4000/verify
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && pnpm --filter @fed-ai/router dev",
      ]
    volumes:
      - keys:/keys:ro
    ports:
      - "18080:8080"

  node:
    build:
      context: ../..
      dockerfile: services/node/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      ln-adapter:
        condition: service_started
      model:
        condition: service_completed_successfully
      router:
        condition: service_started
      llama:
        condition: service_started
    environment:
      NODE_ID: node-llm
      NODE_ENDPOINT: http://node:8081
      NODE_PORT: 8081
      NODE_REQUIRE_PAYMENT: "true"
      NODE_LN_VERIFY_URL: http://ln-adapter:4000/verify
      NODE_RUNNER: llama_cpp
      NODE_LLAMA_CPP_URL: http://llama:8080
      NODE_MODEL_ID: tinyllama
      ROUTER_ENDPOINT: http://router:8080
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && pnpm --filter @fed-ai/node dev",
      ]
    volumes:
      - keys:/keys:ro

  node_cpu:
    build:
      context: ../..
      dockerfile: services/node/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      ln-adapter:
        condition: service_started
      router:
        condition: service_started
    environment:
      NODE_ID: node-cpu
      NODE_ENDPOINT: http://node-cpu:8082
      NODE_PORT: 8082
      NODE_REQUIRE_PAYMENT: "true"
      NODE_LN_VERIFY_URL: http://ln-adapter:4000/verify
      NODE_RUNNER: cpu
      NODE_MODEL_ID: cpu-stats
      ROUTER_ENDPOINT: http://router:8080
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && export NODE_KEY_ID=$$NODE2_KEY_ID NODE_PRIVATE_KEY_PEM=$$NODE2_PRIVATE_KEY_PEM && pnpm --filter @fed-ai/node dev",
      ]
    volumes:
      - keys:/keys:ro

  llama:
    image: ghcr.io/ggml-org/llama.cpp:${LLAMA_CPP_TAG:-server}
    working_dir: /app
    entrypoint: ["/app/llama-server"]
    command:
      [
        "-m",
        "/models/tinyllama.gguf",
        "-c",
        "2048",
        "--host",
        "0.0.0.0",
        "--port",
        "8080",
      ]
    volumes:
      - ./models:/models:ro
    ports:
      - "18085:8080"

  chat:
    image: node:20-bookworm-slim
    working_dir: /app
    volumes:
      - ./:/app
    environment:
      ROUTER_URL: http://router:8080
      MODEL_ID: auto
      MAX_TOKENS: 128
      WALLET_SATS: 2500
      PORT: 3000
    command: ["node", "server.js"]
    depends_on:
      router:
        condition: service_started
      node:
        condition: service_started
    ports:
      - "3000:3000"

volumes:
  keys:
  ln-adapter:
    image: node:20-bookworm-slim
    working_dir: /app
    volumes:
      - ../../tools/ln-adapter:/app
    environment:
      LN_ADAPTER_PORT: 4000
      LN_ADAPTER_BACKEND: ${LN_ADAPTER_BACKEND:-lnbits}
      LNBITS_URL: ${LNBITS_URL}
      LNBITS_API_KEY: ${LNBITS_API_KEY}
      LND_REST_URL: ${LND_REST_URL}
      LND_MACAROON_HEX: ${LND_MACAROON_HEX}
    command: ["node", "server.js"]
