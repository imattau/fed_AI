services:
  keys:
    image: node:20-bookworm-slim
    working_dir: /tmp
    volumes:
      - ./scripts:/scripts:ro
      - keys:/keys
    command:
      [
        "sh",
        "-c",
        "if [ ! -f /keys/keys.env ]; then npm init -y >/dev/null 2>&1 && npm install nostr-tools@2.7.2 >/dev/null 2>&1 && NODE_PATH=/tmp/node_modules node /scripts/gen-keys.js /keys/keys.env; else echo 'keys already present'; fi",
      ]

  model:
    image: curlimages/curl:8.5.0
    user: "0:0"
    volumes:
      - ./models:/models
    environment:
      MODEL_URL: https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf
    entrypoint:
      [
        "sh",
        "-c",
        "if [ ! -f /models/tinyllama.gguf ]; then echo 'downloading model'; curl -L \"$$MODEL_URL\" -o /models/tinyllama.gguf; else echo 'model already present'; fi",
      ]

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: fedai
      POSTGRES_USER: fedai
      POSTGRES_PASSWORD: fedai
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U fedai -d fedai"]
      interval: 5s
      timeout: 3s
      retries: 10

  ln-adapter:
    image: node:20-bookworm-slim
    working_dir: /app
    volumes:
      - ../../tools/ln-adapter:/app
      - /app/node_modules
    environment:
      LN_ADAPTER_PORT: 4000
      LN_ADAPTER_BACKEND: ${LN_ADAPTER_BACKEND:-mock}
      LNBITS_URL: ${LNBITS_URL}
      LNBITS_API_KEY: ${LNBITS_API_KEY}
      LND_REST_URL: ${LND_REST_URL}
      LND_MACAROON_HEX: ${LND_MACAROON_HEX}
    command: ["sh", "-c", "npm install && node server.js"]

  router:
    build:
      context: ../..
      dockerfile: services/router/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      ln-adapter:
        condition: service_started
    environment:
      ROUTER_ENDPOINT: http://router:8080
      ROUTER_PORT: 8080
      ROUTER_REQUIRE_PAYMENT: "true"
      ROUTER_WORKER_THREADS_ENABLED: "true"
      ROUTER_WORKER_THREADS_MAX: "2"
      ROUTER_WORKER_THREADS_QUEUE_MAX: "200"
      ROUTER_WORKER_THREADS_TIMEOUT_MS: "2000"
      ROUTER_LN_INVOICE_URL: http://ln-adapter:4000/invoice
      ROUTER_LN_VERIFY_URL: http://ln-adapter:4000/verify
      ROUTER_DB_URL: postgres://fedai:fedai@postgres:5432/fedai
      ROUTER_DB_SSL: "false"
      ROUTER_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      ROUTER_FEE_ENABLED: "true"
      ROUTER_FEE_SPLIT: "true"
      ROUTER_FEE_BPS: "500"
      ROUTER_FEE_FLAT_SATS: "0"
      ROUTER_FEE_MIN_SATS: "0"
      ROUTER_FEE_MAX_SATS: "500"
      ROUTER_FEDERATION_ENABLED: "true"
      ROUTER_FEDERATION_RATE_LIMIT_MAX: "60"
      ROUTER_FEDERATION_RATE_LIMIT_WINDOW_MS: "10000"
      ROUTER_FEDERATION_NOSTR: "true"
      ROUTER_FEDERATION_NOSTR_RELAYS: "wss://relay.damus.io,wss://relay.snort.social"
      ROUTER_FEDERATION_NOSTR_RETRY_MIN_MS: "1000"
      ROUTER_FEDERATION_NOSTR_RETRY_MAX_MS: "30000"
      ROUTER_ADMIN_KEY: "admin-secret"
      ROUTER_ADMIN_NPUB: ${ROUTER_ADMIN_NPUB:-}
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && export ROUTER_FEDERATION_NOSTR_FOLLOW=$$ROUTER_KEY_ID && pnpm --filter @fed-ai/router dev",
      ]
    volumes:
      - keys:/keys:ro
    ports:
      - "18080:8080"

  node:
    build:
      context: ../..
      dockerfile: services/node/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      ln-adapter:
        condition: service_started
      model:
        condition: service_completed_successfully
      router:
        condition: service_started
      llama:
        condition: service_started
    environment:
      NODE_ID: node-llm
      NODE_ENDPOINT: http://node:8081
      NODE_PORT: 8081
      NODE_REQUIRE_PAYMENT: "true"
      NODE_WORKER_THREADS_ENABLED: "true"
      NODE_WORKER_THREADS_MAX: "2"
      NODE_WORKER_THREADS_QUEUE_MAX: "200"
      NODE_WORKER_THREADS_TIMEOUT_MS: "2000"
      NODE_LN_VERIFY_URL: http://ln-adapter:4000/verify
      NODE_RUNNER: llama_cpp
      NODE_LLAMA_CPP_URL: http://llama:8080
      NODE_MODEL_ID: tinyllama
      ROUTER_ENDPOINT: http://router:8080
      NODE_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      NODE_ROUTER_FEE_MAX_BPS: "1000"
      NODE_ROUTER_FEE_MAX_SATS: "1000"
      NODE_ADMIN_KEY: "admin-secret"
      NODE_ADMIN_NPUB: ${NODE_ADMIN_NPUB:-}
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && export NODE_ROUTER_FOLLOW=$$ROUTER_KEY_ID && pnpm --filter @fed-ai/node dev",
      ]
    volumes:
      - keys:/keys:ro

  node_cpu:
    build:
      context: ../..
      dockerfile: services/node/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      ln-adapter:
        condition: service_started
      router:
        condition: service_started
    environment:
      NODE_ID: node-cpu
      NODE_ENDPOINT: http://node_cpu:8082
      NODE_PORT: 8082
      NODE_REQUIRE_PAYMENT: "true"
      NODE_WORKER_THREADS_ENABLED: "true"
      NODE_WORKER_THREADS_MAX: "2"
      NODE_WORKER_THREADS_QUEUE_MAX: "200"
      NODE_WORKER_THREADS_TIMEOUT_MS: "2000"
      NODE_LN_VERIFY_URL: http://ln-adapter:4000/verify
      NODE_RUNNER: cpu
      NODE_MODEL_ID: cpu-stats
      ROUTER_ENDPOINT: http://router:8080
      NODE_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      NODE_ROUTER_FEE_MAX_BPS: "1000"
      NODE_ROUTER_FEE_MAX_SATS: "1000"
      NODE_ADMIN_KEY: "admin-secret"
      NODE_ADMIN_NPUB: ${NODE_ADMIN_NPUB:-}
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && export NODE_KEY_ID=$$NODE2_KEY_ID NODE_PRIVATE_KEY_PEM=$$NODE2_PRIVATE_KEY_PEM NODE_ROUTER_FOLLOW=$$ROUTER_KEY_ID && pnpm --filter @fed-ai/node dev",
      ]
    volumes:
      - keys:/keys:ro

  node_grok:
    build:
      context: ../..
      dockerfile: services/node/Dockerfile
    depends_on:
      keys:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      ln-adapter:
        condition: service_started
      router:
        condition: service_started
    environment:
      NODE_ID: node-grok
      NODE_ENDPOINT: http://node_grok:8083
      NODE_PORT: 8083
      NODE_REQUIRE_PAYMENT: "true"
      NODE_WORKER_THREADS_ENABLED: "true"
      NODE_WORKER_THREADS_MAX: "2"
      NODE_WORKER_THREADS_QUEUE_MAX: "200"
      NODE_WORKER_THREADS_TIMEOUT_MS: "2000"
      NODE_LN_VERIFY_URL: http://ln-adapter:4000/verify
      NODE_RUNNER: openai
      NODE_OPENAI_URL: https://api.groq.com/openai
      NODE_OPENAI_MODEL: llama-3.1-8b-instant
      NODE_MODEL_ID: llama-3.1-8b-instant
      NODE_OPENAI_API_KEY_HEADER: both
      NODE_EXPOSE_ERRORS: "true"
      ROUTER_ENDPOINT: http://router:8080
      NODE_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      NODE_ADMIN_KEY: "admin-secret"
      NODE_ADMIN_NPUB: ${NODE_ADMIN_NPUB:-}
    entrypoint:
      [
        "sh",
        "-c",
        "export $$(cat /keys/keys.env | xargs) && export NODE_KEY_ID=$$NODE3_KEY_ID NODE_PRIVATE_KEY_PEM=$$NODE3_PRIVATE_KEY_PEM NODE_ROUTER_FOLLOW=$$ROUTER_KEY_ID && pnpm --filter @fed-ai/node dev",
      ]
    volumes:
      - keys:/keys:ro

  llama:
    image: ghcr.io/ggml-org/llama.cpp:${LLAMA_CPP_TAG:-server}
    working_dir: /app
    entrypoint: ["/app/llama-server"]
    command:
      [
        "-m",
        "/models/tinyllama.gguf",
        "-c",
        "2048",
        "--host",
        "0.0.0.0",
        "--port",
        "8080",
      ]
    volumes:
      - ./models:/models:ro
    ports:
      - "18085:8080"

  chat:
    image: node:20-bookworm-slim
    working_dir: /repo
    volumes:
      - ../../:/repo
      - keys:/keys:ro
    environment:
      CI: "1"
      PNPM_CONFIG_CONFIRM_MODULES_DIR: "false"
      COREPACK_ENABLE_DOWNLOAD_PROMPT: "0"
      COREPACK_ENABLE_STRICT: "0"
      ROUTER_URL: http://router:8080
      MODEL_ID: auto
      MAX_TOKENS: 128
      WALLET_SATS: 2500
      PORT: 3000
      ROUTER_DB_URL: postgres://fedai:fedai@postgres:5432/fedai
      ROUTER_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      NODE_NONCE_STORE_URL: postgres://fedai:fedai@postgres:5432/fedai
      ROUTER_FEDERATION_ENABLED: "true"
      ROUTER_FEDERATION_RATE_LIMIT_MAX: "60"
      ROUTER_FEDERATION_RATE_LIMIT_WINDOW_MS: "10000"
      ROUTER_FEDERATION_NOSTR: "true"
      ROUTER_FEDERATION_NOSTR_RELAYS: "wss://relay.damus.io,wss://relay.snort.social"
      ROUTER_FEDERATION_NOSTR_RETRY_MIN_MS: "1000"
      ROUTER_FEDERATION_NOSTR_RETRY_MAX_MS: "30000"
      NODE_ROUTER_FOLLOW: "ROUTER_KEY_ID"
    command:
      [
        "sh",
        "-c",
        "set -eux; corepack enable; corepack prepare pnpm@9.0.0 --activate; pnpm -w install --no-frozen-lockfile --config.confirmModulesDir=false; pnpm --filter @fed-ai/protocol --filter @fed-ai/nostr-relay-discovery --filter @fed-ai/sdk-js run build; node examples/simple-chat/server.js",
      ]
    depends_on:
      router:
        condition: service_started
      node:
        condition: service_started
    ports:
      - "3000:3000"

  admin:
    image: node:20-bookworm-slim
    working_dir: /repo
    volumes:
      - ../../:/repo
    environment:
      PORT: 3001
    command:
      [
        "sh",
        "-c",
        "cd examples/admin-dashboard && npm install && node server.js",
      ]
    ports:
      - "3001:3001"


volumes:
  keys:
  pgdata:
